{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contents\n",
    "\n",
    "This notebook shows two retrieval examples\n",
    "- Retrieving by rplan id based on precomputed features\n",
    "- Computing embeddings for a new image, and retrieving rplan images similar to it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import datasets\n",
    "\n",
    "class FloorPlanImages:\n",
    "    def __init__(self, ds_img) -> None:\n",
    "        self.ds_img = ds_img\n",
    "\n",
    "        self.id_to_index = {}\n",
    "\n",
    "        for i, id in enumerate(self.ds_img[\"id\"]):\n",
    "            self.id_to_index[id] = i\n",
    "    \n",
    "    def __getitem__(self, id):\n",
    "        return datasets.Image(decode=True).decode_example(self.ds_img[self.id_to_index[id]][\"img\"])\n",
    "    \n",
    "\n",
    "class VisualizeRetrievals:\n",
    "\n",
    "    def __init__(self, images: FloorPlanImages):\n",
    "        self.images = images\n",
    "\n",
    "    def visualize_query_by_id(self, query_id, retrieved_ids, titles=None, relevants=None):\n",
    "        \"\"\"Visualize the search results.\n",
    "        \n",
    "        First item of the lists is the query.\"\"\"\n",
    "\n",
    "        k = len(retrieved_ids) + 1\n",
    "\n",
    "        # if axes is None:\n",
    "        fig, axes = plt.subplots(1, k, dpi=150, figsize=(20 * k / 5 * 0.75, 7.5 * 0.75))\n",
    "        fig.tight_layout(pad=1.0)\n",
    "\n",
    "        axes[0].imshow(self.images[query_id])\n",
    "\n",
    "        axes[0].set_title(f\"{query_id=}\")\n",
    "        axes[0].axis(\"off\")\n",
    "\n",
    "        for i, id in enumerate(retrieved_ids):\n",
    "\n",
    "            axes[i+1].imshow(self.images[id])\n",
    "\n",
    "            if titles is None:\n",
    "                axes[i + 1].set_title(f\"{id=}\")\n",
    "            else:\n",
    "                axes[i + 1].set_title(f\"{id=}\\n{titles[i]}\")\n",
    "\n",
    "            axes[i + 1].axis(\"off\")\n",
    "\n",
    "        return fig\n",
    "\n",
    "\n",
    "    def visualize_query_by_image(self, retrieved_ids, query_img, titles=None, query_img_title=\"query\"):\n",
    "        \"\"\"Visualize the search results.\n",
    "        \n",
    "        First item of the lists is the query.\"\"\"\n",
    "\n",
    "        k = len(retrieved_ids) + 1\n",
    "\n",
    "        # if axes is None:\n",
    "        fig, axes = plt.subplots(1, k, dpi=150, figsize=(20 * k / 5 * 0.75, 7.5 * 0.75))\n",
    "        fig.tight_layout(pad=1.0)\n",
    "\n",
    "        axes[0].axis(\"off\")\n",
    "        axes[0].imshow(query_img)\n",
    "        axes[0].set_title(query_img_title)\n",
    "\n",
    "        for i, id in enumerate(retrieved_ids):\n",
    "\n",
    "            img = self.images[id]            \n",
    "\n",
    "            axes[i+1].imshow(img)\n",
    "\n",
    "            if titles is None:\n",
    "                axes[i + 1].set_title(f\"{id=}\")\n",
    "            else:\n",
    "                axes[i + 1].set_title(f\"{id=}\\n{titles[i]}\")\n",
    "\n",
    "            axes[i + 1].axis(\"off\")\n",
    "\n",
    "        return fig\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code for loading precomputed features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import os\n",
    "import typing\n",
    "\n",
    "def infer_dataset_name(wandb_prefix, wandb_model_ref, features_cache_folder=\"data/predicted/\"):\n",
    "    model_folder = Path(features_cache_folder) / wandb_prefix / wandb_model_ref\n",
    "\n",
    "    dataset_name_options = [item for item in os.listdir(model_folder) if os.path.isdir(model_folder / item)]\n",
    "\n",
    "    if len(dataset_name_options) == 0:\n",
    "        raise RuntimeError(f\"No features found for {wandb_prefix}/{wandb_model_ref}. First run predict_embeddings.py\")\n",
    "\n",
    "    if len(dataset_name_options) > 1:\n",
    "        raise RuntimeError(f\"More than one dataset_name option was found, specify one of {dataset_name_options}\")\n",
    "\n",
    "    dataset_name = dataset_name_options[0]\n",
    "\n",
    "    return dataset_name\n",
    "\n",
    "\n",
    "def load_features(wandb_prefix, wandb_model_ref, split, dataset_name, features_cache_folder=\"data/predicted/\") -> typing.Dict[str, torch.Tensor]:\n",
    "\n",
    "    model_folder = Path(features_cache_folder) / wandb_prefix / wandb_model_ref\n",
    "\n",
    "    split_options = [item for item in os.listdir(model_folder / dataset_name) if os.path.isdir(model_folder / dataset_name / item)]\n",
    "\n",
    "    assert split in split_options, f\"Did not find features for {split=}, only for {split_options}\"\n",
    "\n",
    "    features_path = Path(features_cache_folder) / wandb_prefix / wandb_model_ref / dataset_name / split / \"feats.pth\"\n",
    "    \n",
    "    return torch.load(features_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieve based on an rplan id in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from inference.retrieval_context import EmbeddingsRetrievalContext\n",
    "from inference.model_context import WandbModelContext\n",
    "\n",
    "retrieval_context = None\n",
    "\n",
    "wandb_prefix = \"emanuel/msc_thesis_models\"\n",
    "wandb_model_ref = \"model-uc18eq89:best\"\n",
    "split = \"val\"\n",
    "\n",
    "dataset_name = infer_dataset_name(wandb_prefix, wandb_model_ref)\n",
    "features = load_features(wandb_prefix, wandb_model_ref, split, dataset_name=dataset_name)\n",
    "\n",
    "retrieval_context = EmbeddingsRetrievalContext(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If it failed, first preprocess the dataset and precompute features:\n",
    "\n",
    "if retrieval_context is None:\n",
    "\n",
    "    from predict_embeddings import model_huggingface_url_to_preprocessing_style\n",
    "\n",
    "    model_context = WandbModelContext(wandb_prefix, wandb_model_ref)\n",
    "    method = model_huggingface_url_to_preprocessing_style(model_context.huggingface_dataset)\n",
    "\n",
    "    rplan_dataset_path = \"/home/emanuel/thesisdata/dataset/floorplan_dataset\"\n",
    "\n",
    "    # Preprocess dataset:\n",
    "    if not os.path.exists(f\"data/processed/{method.value}/{split}\"):\n",
    "        print(f\"Run: python run_preprocessing.py method={method.name} rplan_dataset_path={rplan_dataset_path} split={split}\")\n",
    "        !python run_preprocessing.py method={method.name} rplan_dataset_path={rplan_dataset_path} split={split}\n",
    "\n",
    "    # Precompute features:\n",
    "    print(f\"Run: python predict_embeddings.py wandb_prefix={wandb_prefix} wandb_model_ref={wandb_model_ref} split={split}\")\n",
    "    !python predict_embeddings.py wandb_prefix={wandb_prefix} wandb_model_ref={wandb_model_ref} split={split}\n",
    "\n",
    "# Now exectute the cell above again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By default uses the same dataset as the model, you can set it to:\n",
    "# dataset_name = \"ds_rplanpy_rgb\"\n",
    "# for nicer visualizations\n",
    "ds_images = datasets.load_from_disk(f\"data/processed/{dataset_name}/{split}/\")\n",
    "\n",
    "visualize_retrievals = VisualizeRetrievals(FloorPlanImages(ds_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_top_k = 5\n",
    "\n",
    "query_ids = [4]\n",
    "\n",
    "assert retrieval_context is not None\n",
    "\n",
    "retrieved_idss = retrieval_context.retrieve(query_ids)[:, :show_top_k]\n",
    "\n",
    "for query_id, retrieved_ids in zip(query_ids, retrieved_idss):\n",
    "    plot = visualize_retrievals.visualize_query_by_id(query_id, retrieved_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieve based on a new image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from inference.model_context import WandbModelContext\n",
    "\n",
    "model_ctx = WandbModelContext(\"emanuel/msc_thesis_models\", \"model-uc18eq89:best\")\n",
    "\n",
    "model_ctx.model.cpu()\n",
    "model = model_ctx.model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# Open an image to use as query\n",
    "query_img = Image.open(\"data/example_edited_query_images/rplan-0-squared-version.png\").convert(\"RGB\")\n",
    "\n",
    "query_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.array(query_img).shape == (256, 256, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from torchvision.transforms.functional import to_tensor\n",
    "\n",
    "with torch.no_grad():\n",
    "    repr_vector = model_ctx.model({\"img\": to_tensor(query_img).unsqueeze(0)})\n",
    "\n",
    "query_embeddings = repr_vector[\"pred\"]\n",
    "\n",
    "query_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from inference.retrieval_context import EmbeddingsRetrievalContext\n",
    "\n",
    "wandb_prefix = \"emanuel/msc_thesis_models\"\n",
    "wandb_model_ref = \"model-uc18eq89:best\"\n",
    "split = \"val\"\n",
    "\n",
    "dataset_name = infer_dataset_name(wandb_prefix, wandb_model_ref)\n",
    "features = load_features(wandb_prefix, wandb_model_ref, split, dataset_name=dataset_name)\n",
    "\n",
    "retrieval_context = EmbeddingsRetrievalContext(features)\n",
    "\n",
    "ds_images = datasets.load_from_disk(f\"data/processed/{dataset_name}/{split}/\")\n",
    "\n",
    "visualize_retrievals = VisualizeRetrievals(FloorPlanImages(ds_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_ids = retrieval_context.retrieve_by_embedding(query_embeddings, top_k=100)[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = visualize_retrievals.visualize_query_by_image(retrieved_ids, query_img)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
